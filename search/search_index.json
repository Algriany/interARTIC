{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to InterARTIC InterARTIC is web application designed to automate the use of the Conda ARTIC pipeline. It is designed for use for the Nanopolish and Medaka pipelines.","title":"Home"},{"location":"#welcome-to-interartic","text":"InterARTIC is web application designed to automate the use of the Conda ARTIC pipeline. It is designed for use for the Nanopolish and Medaka pipelines.","title":"Welcome to InterARTIC"},{"location":"installation/","text":"Installation Installation Dependencies Opening terminal Installing Python and pip Installing miniconda Installing the ARTIC pipeline environment Installing the Redis Server and Celery Installing Python packages for Redis, Celery and Flask Installing interARTIC Setting Up interARTIC Job Concurrency Automatic setup Manual setup Concurrency Manual setup Running interARTIC Dependencies Python 3.7 or above Celery v4.4.6 (cliffs) Redis Server v6.0.5 Miniconda Opening terminal Your operating system's command line will be used to install the dependencies and start interARTIC. For Mac OS: Type \u201cterminal\u201d into your spotlight search, then hit Return. For Windows: Type \u201ccmd\u201d into your search bar in the Start menu, then hit Enter. For Linux: Enter the keyboard shortcut: Ctrl+Alt+T. If these instructions don't work on your operating system, google how to open command line on your operating system and software version. Installing Python and pip In order to use interARTIC, you\u2019ll need Python and its package manager pip installed on your system. Check if they're already installed by entering the following into your command prompt: python --version pip --version If Python is not installed, go to: https://www.python.org/downloads/ and follow the instructions there. If you are a Linux user, you may instead follow the commands below to install Python: sudo add-apt-repository ppa:deadsnakes/ppa sudo apt-get update sudo apt install python3.7 python3.7-dev python3.7-venv If you have just installed Python, it will likely have also installed pip. Check that it is installed, and upgrade if necessary using the 2nd command below. pip --version pip install --upgrade pip If pip is not installed, go to: https://pip.pypa.io/en/stable/installing/ and follow the prompts there. Installing miniconda The miniconda installation guide can be found here: https://conda.io/projects/conda/en/latest/user-guide/install/index.htm We suggest you undergo the 'Regular Installation' process. Installing the ARTIC pipeline environment Enter the following into your command prompt: git clone https://github.com/artic-network/artic-ncov2019.git cd artic-ncov2019 conda env remove -n artic-ncov2019 conda env create -f environment.yml Installing the Redis Server and Celery Follow this link to install the Redis Server https://redis.io/topics/quickstart , then enter the following into your command prompt: bash run-redis.sh Installing Python packages for Redis, Celery and Flask To install the Python packages for Redis, Celery and Flask, enter the following into your command prompt: pip3 install celery==4.4.6 redis==3.5.3 flask Installing interARTIC Clone the repository from github by entering the following commands into your terminal. git clone https://github.com/tthnguyen11/SARS-CoV-2-NanoporeAnalysisWebApp.git Setting Up interARTIC Job Concurrency By default, job concurrency is turned off and the automatic and manual setups will allow one job to be run at a time. If you wish to turn concurrency on and run multiple jobs at a time, then please run the commands under the Concurrency Manual setup heading, which will allow all the CPUs available on your machine to be used to run jobs. Note that running jobs concurrently will likely slow down the speed of your machine. Automatic setup To start interARTIC, navigate to the directory where the repository was cloned and enter the following command into your command prompt: cd SARS-CoV-2-NanoporeAnalysisWebApp bash run.sh <terminal type> # Terminal types: macos, xterm, konsole Manual setup If your terminal is not listed, enter the following commands into your command prompt: cd SARS-CoV-2-NanoporeAnalysisWebApp bash run-redis.sh & conda activate artic-ncov2019; celery worker -A main.celery --concurrency=1 --loglevel=info & python3 main.py Concurrency Manual setup If you wish to turn job concurrency on, enter the following commands into your command prompt: cd SARS-CoV-2-NanoporeAnalysisWebApp bash run-redis.sh & conda activate artic-ncov2019; celery worker -A main.celery --loglevel=info & python3 main.py Running interARTIC Navigate to your browser and go to http://127.0.0.1:5000 to access interARTIC.","title":"Installation"},{"location":"installation/#installation","text":"Installation Dependencies Opening terminal Installing Python and pip Installing miniconda Installing the ARTIC pipeline environment Installing the Redis Server and Celery Installing Python packages for Redis, Celery and Flask Installing interARTIC Setting Up interARTIC Job Concurrency Automatic setup Manual setup Concurrency Manual setup Running interARTIC","title":"Installation"},{"location":"installation/#dependencies","text":"Python 3.7 or above Celery v4.4.6 (cliffs) Redis Server v6.0.5 Miniconda","title":"Dependencies"},{"location":"installation/#opening-terminal","text":"Your operating system's command line will be used to install the dependencies and start interARTIC. For Mac OS: Type \u201cterminal\u201d into your spotlight search, then hit Return. For Windows: Type \u201ccmd\u201d into your search bar in the Start menu, then hit Enter. For Linux: Enter the keyboard shortcut: Ctrl+Alt+T. If these instructions don't work on your operating system, google how to open command line on your operating system and software version.","title":"Opening terminal"},{"location":"installation/#installing-python-and-pip","text":"In order to use interARTIC, you\u2019ll need Python and its package manager pip installed on your system. Check if they're already installed by entering the following into your command prompt: python --version pip --version If Python is not installed, go to: https://www.python.org/downloads/ and follow the instructions there. If you are a Linux user, you may instead follow the commands below to install Python: sudo add-apt-repository ppa:deadsnakes/ppa sudo apt-get update sudo apt install python3.7 python3.7-dev python3.7-venv If you have just installed Python, it will likely have also installed pip. Check that it is installed, and upgrade if necessary using the 2nd command below. pip --version pip install --upgrade pip If pip is not installed, go to: https://pip.pypa.io/en/stable/installing/ and follow the prompts there.","title":"Installing Python and pip"},{"location":"installation/#installing-miniconda","text":"The miniconda installation guide can be found here: https://conda.io/projects/conda/en/latest/user-guide/install/index.htm We suggest you undergo the 'Regular Installation' process.","title":"Installing miniconda"},{"location":"installation/#installing-the-artic-pipeline-environment","text":"Enter the following into your command prompt: git clone https://github.com/artic-network/artic-ncov2019.git cd artic-ncov2019 conda env remove -n artic-ncov2019 conda env create -f environment.yml","title":"Installing the ARTIC pipeline environment"},{"location":"installation/#installing-the-redis-server-and-celery","text":"Follow this link to install the Redis Server https://redis.io/topics/quickstart , then enter the following into your command prompt: bash run-redis.sh","title":"Installing the Redis Server and Celery"},{"location":"installation/#installing-python-packages-for-redis-celery-and-flask","text":"To install the Python packages for Redis, Celery and Flask, enter the following into your command prompt: pip3 install celery==4.4.6 redis==3.5.3 flask","title":"Installing Python packages for Redis, Celery and Flask"},{"location":"installation/#installing-interartic","text":"Clone the repository from github by entering the following commands into your terminal. git clone https://github.com/tthnguyen11/SARS-CoV-2-NanoporeAnalysisWebApp.git","title":"Installing interARTIC"},{"location":"installation/#setting-up-interartic","text":"","title":"Setting Up interARTIC"},{"location":"installation/#job-concurrency","text":"By default, job concurrency is turned off and the automatic and manual setups will allow one job to be run at a time. If you wish to turn concurrency on and run multiple jobs at a time, then please run the commands under the Concurrency Manual setup heading, which will allow all the CPUs available on your machine to be used to run jobs. Note that running jobs concurrently will likely slow down the speed of your machine.","title":"Job Concurrency"},{"location":"installation/#automatic-setup","text":"To start interARTIC, navigate to the directory where the repository was cloned and enter the following command into your command prompt: cd SARS-CoV-2-NanoporeAnalysisWebApp bash run.sh <terminal type> # Terminal types: macos, xterm, konsole","title":"Automatic setup"},{"location":"installation/#manual-setup","text":"If your terminal is not listed, enter the following commands into your command prompt: cd SARS-CoV-2-NanoporeAnalysisWebApp bash run-redis.sh & conda activate artic-ncov2019; celery worker -A main.celery --concurrency=1 --loglevel=info & python3 main.py","title":"Manual setup"},{"location":"installation/#concurrency-manual-setup","text":"If you wish to turn job concurrency on, enter the following commands into your command prompt: cd SARS-CoV-2-NanoporeAnalysisWebApp bash run-redis.sh & conda activate artic-ncov2019; celery worker -A main.celery --loglevel=info & python3 main.py","title":"Concurrency Manual setup"},{"location":"installation/#running-interartic","text":"Navigate to your browser and go to http://127.0.0.1:5000 to access interARTIC.","title":"Running interARTIC"},{"location":"troubleshooting/","text":"Troubleshooting Troubleshooting Submitted job not running Job not running after submission Usage errors Missing job on Home Page Any other errors Submitted job not running Job not running after submission This may be because there are too many Celery tasks running in the background. To rectify this, enter: ps aux | grep celery kill -9 <process ID> # where process ID is the number in the second column # repeat this until all celery processes have been removed Usage errors If a usage error is printing in the progress page, this may mean that a particular command has not been implemented, or the read file you have submitted has not been formatted correctly. An example of a usage error is: usage: artic [-h] [-v] To rectify this, first check that the read file you have submitted has been formatted correctly i.e. with no header. More information can be found on the usage page of this documentation. If this does not fix the issue, please let the developers know via GitHub so that the command can be implemented. Missing job on Home Page If a job is missing from the home page, wait a few seconds and refresh the page. Any other errors If there are any other errors, please let the developers know via GitHub so that we can look into it for you.","title":"Troubleshooting"},{"location":"troubleshooting/#troubleshooting","text":"Troubleshooting Submitted job not running Job not running after submission Usage errors Missing job on Home Page Any other errors","title":"Troubleshooting"},{"location":"troubleshooting/#submitted-job-not-running","text":"","title":"Submitted job not running"},{"location":"troubleshooting/#job-not-running-after-submission","text":"This may be because there are too many Celery tasks running in the background. To rectify this, enter: ps aux | grep celery kill -9 <process ID> # where process ID is the number in the second column # repeat this until all celery processes have been removed","title":"Job not running after submission"},{"location":"troubleshooting/#usage-errors","text":"If a usage error is printing in the progress page, this may mean that a particular command has not been implemented, or the read file you have submitted has not been formatted correctly. An example of a usage error is: usage: artic [-h] [-v] To rectify this, first check that the read file you have submitted has been formatted correctly i.e. with no header. More information can be found on the usage page of this documentation. If this does not fix the issue, please let the developers know via GitHub so that the command can be implemented.","title":"Usage errors"},{"location":"troubleshooting/#missing-job-on-home-page","text":"If a job is missing from the home page, wait a few seconds and refresh the page.","title":"Missing job on Home Page"},{"location":"troubleshooting/#any-other-errors","text":"If there are any other errors, please let the developers know via GitHub so that we can look into it for you.","title":"Any other errors"},{"location":"usage/","text":"Usage Usage Adding a job Parameters File path input Input directory file structure Parameter Descriptions Progress Page What happens if an error occurs during the run? What happens when a job is completed? Output Page Files Produced Data Visualisation Variants Found Plots Produced From Pipeline Adding a job To begin the process of adding a job, click the 'Add Job' button located underneath the Jobs Queue on the home page. Parameters Input the necessary parameters (see Parameter Descriptions below). Parameters required for any type of job run are denoted with an asterix (*). For the file path inputs, please enter absolute paths. See below for help. File path input Folders and files should be inputted by their file paths. File paths can be retrieved by running 'pwd' in the appropriate folder on any terminal. Folders may also be referred to as directories. File paths should start with \u201c/\u201d (Mac or Linux) or \u201cC:\\\u201d (Windows). If you have not worked with navigating folders and files in the terminal before, take a look at this resource: https://www.earthdatascience.org/courses/intro-to-earth-data-science/python-code-fundamentals/work-with-files-directories-paths-in-python/. For example: $ pwd # get file path of current directory /Users/YOURNAME $ ls # list contents of current directory folder1 folder2 file1 documents $ cd documents # change current directory to documents $ pwd /Users/YOURNAME/documents $ cd inputFolder # change current directory to your input folder $ ls # check contents of folder are correct fast5_fail fast5_pass fastq_fail fastq_pass sample-barcode.csv sequencing_summary.txt $ pwd # obtain file path you will input into interARTIC /Users/YOURNAME/documents/inputFolder Note: your input folder may not be located in documents folder. Simply navigate, using these commands, to inside your input folder and obtain the file path. Input directory file structure You must rename each folder/ file in the input folder to: fast5_pass, fast5_fail, fastq_pass, fastq_fail, sequencing_summary.txt, sample-barcode.csv If a single sample is being run through the pipeline: If available, the file path for a read file should be inputted. If unavailable, the artic gather/demultiplex command will generate one. If multiple samples are being run through the pipeline: A CSV file containing sample names and barcodes should be placed in the input folder and named \u2018sample-barcode.csv\u2019. A sample CSV file is below: A sample file structure is as below: input_directory/ fast5_pass/ A_10.fast5 A_11.fast5 fast5_fail/ A_0.fast5 A_1.fast5 fastq_pass/ B_0.fastq B_1.fastq fastq_fail/ B_10.fastq B_11.fastq sample-barcode.csv sequencing_summary.txt readfile.fastq #optional file primer-schemes/ nCoV-2019/ V1/ nCoV-2019.log nCoV-2019.pdf nCoV-2019.pickle nCoV-2019.reference.fasta nCoV-2019.reference.fasta.fai nCoV-2019.scheme.bed nCoV-2019.svg nCoV-2019.tsv Parameter Descriptions You can customize the parameters by typing into the respective text box. Job name: A unique name for your job, so you may identify your output files with it. Pipeline: Select the pipeline within ARTIC that you wish to run your data files through. Input folder: Enter the file path to your main data folder. This folder contains folders such as fast5_pass, fastq_pass, etc. Read file: If you are inputting data files for a single sample run where you have a file ending in \u201c.fastq\u201d already made, input the file path for this read file. You may get this error in your output, but this can be ignored: Include screenshot of harmless error that might occur Output folder: If your chosen output folder is already created, enter the file path for this folder. Otherwise, you may enter a file path to a folder that doesn\u2019t exist yet. If you do this, ensure that the parent directory exists. For example, if you are inputting this file path as your output folder: path/to/file/hello/world, the folder \u201chello\u201d must already exist for the \u201cworld\u201d folder to be created. If this is not inputted, an output folder will be generated within the input folder. Primer scheme folder: Enter the file path to your primer schemes. This is the folder containing, for example, the folder nCoV-2019 which contains the V1, V2, etc folders. Primer scheme name: Enter the primer scheme name used for your nanopore sequencing run. Following a similar example to the previous parameter description, here you will enter a path such as nCoV-2019/V1. Primer/Barcode type: Enter the type of primer/barcode used. Either select from the options available or enter the name of the primer/barcode in the text box. This is only used for folder-naming purposes. Minimum/Maximum length: If you selected from the available options in the primer/barcode type section, you may find the minimum and maximum length already filled out. If not, set this to the minimum/maximum length of your primers. Thread usage/Normalise: Change the prefilled values if you wish. Please note that changing the threads/normalise values, they are changed globally for all commands. Single or Multiple samples: Select the appropriate option. When you are confident that your parameter selections are correct, click on the \u201cSubmit Job(s)\u201d button. You will be redirected to the progress page after clicking this button. Progress Page The progress page displays the stream of standard output being produced by your job run. Here you can see which commands are currently running and any errors that occur. Each job run has its own progress page which can be accessed via the home page or parameters page by clicking on the job name in the jobs queue. For each job, the progress page will display: The job name The job's place in queue An 'Abort Job' button The overall progress of the job in the form of a progress bar and the number of steps remaining in the pipeline A 'View Job Parameters here' button The current standard output obtained from the job The 'View Job Parameters here' button, when clicked, will display the job's parameters that have been entered by the user. The 'Abort Job' button can be used to terminate the job. A confirmation window will appear when you click on the abort button. If you continue, you will then be asked to confirm whether you wish to delete the files created by the job. After this, you will then be directed back to the home page. What happens if an error occurs during the run? If an error occurs during a run, a red notification will appear. You can either let the job continue to run, or click the \u2018Re-run\u2019 button. Harmless errors sometimes occur in the ARTIC pipeline, so it may be worth waiting for the run to finish and then assessing your output. Clicking the \u2018Re-run\u2019 button will allow you to abort the currently running job and re-run the job with editted parameters. A confirmation window will appear when you click on the \u2018Re-run\u2019 button asking you to confirm that you wish to abort the current job and whether to delete the files created by the job. You will then be redirected to the parameters page where the information from the job in question will be automatically filled in. You can make any changes necessary, and the new job will be added to the end of the queue following submission. What happens when a job is completed? When a job is completed, a \u2018Go to Output\u2019 button will appear at the top of the page. Click the button to be redirected to the output page. The job will also be moved to the 'Completed Jobs' list on the home page where you can click on the job name and be redirected to the output page for that job. Output Page The Output Page has two main sections, one for the files produced during the run, and the other for data visualisation to enable a fast quality check of the sample. At the bottom of the page, there is a 'Go to Progress' button which will redirect you to the progress page of the job if you click on it. Files Produced This section contains a list of the files produced from the job, along with the location in which they were saved. This location should be the output folder that you inputted on the parameters page, or if you did not input one, it will be a folder named \u201coutput\u201d created inside your input folder. Data Visualisation The \u201cData Visualisation\u201d section comprises two main parts. Variants found Plots produced from pipeline Variants Found This section produces a simple graph/s from the data inside the <sample_name>.pass.vcf.gz file/s produced by the pipeline. If multiple samples are selected, multiple files will be produced (assuming all runs are successful). The 'Produce Graphs' button creates the graph/s. The vertical lines on the graph represent a variant, with the specific numerical position of the variant on the bottom axis. The mutation that occurred is labelled REF -> ALT, meaning that the nucleotide/s in the reference genome are on the left of the arrow, and the variant is on the right of the arrow. The height of the vertical line indicates the read depth of the specific variant. To download the graph, click on the 'Download' hyperlink in the lower-left corner of the graph of interest. These graphs, as mentioned, require interARTIC to read the <sample_name>.pass.vcf.gz file/s from the output folder. If there are no such files found, then no graphs will be produced. If you would not like the web app to access this, please click the \u201cDisable\u201d option under \u201cEnable VCF graphs to be generated?\u201d and click the 'Confirm' button. When you are no longer given the option to produce the graphs, and the status of the \u201cEnable VCF graphs to be generated?\u201d section is \u201cDisabled\u201d this is complete. If you would like to re-enable the graphs to be produced, just select \u201cEnable\u201d and once again confirm. The functionality should be restored. If no <sample_name>.pass.vcf.gz files are found in the output folder, the message \u201cVcf graph could not be made: No pass.vfc.gz file/s found in the output folder.\u201d will be displayed. As no files of the suitable format have been found, these graph/s cannot be produced. This may be due to errors or problems during the pipeline, so checking error messages in the progress page's standard output section is important. Plots Produced From Pipeline This section enables you to preview the mean amplicon read depth plots produced from the pipeline. The bar plot shows the mean amplicon read depth over the amplicons, while the box plot shows the mean amplicon read depth over the read groups. These plots are accessed from the output folder, and require a copy of them to be saved into the interARTIC web app\u2019s folder to enable previewing. If you would not like this to happen, please disable the feature by selecting the \u201cDisable\u201d option under the \u201cWould you like the plots to be previewed?\u201d section, and clicking the 'Confirm' button. The status should be updated to say \u201cDisabled\u201d and the plots will be automatically removed from the interARTIC web app's folder. To re-enable previewing, select \u201cEnable\u201d and press the 'Confirm' button again, and the option to preview graphs should be restored. If no barplot.png or boxplot.png files are found in the output folder, the message \u201cPlots cannot be previewed: No plots were found in the output folder.\u201d will be displayed. As no files in suitable format have been found, these graph/s cannot be previewed. This may be due to errors or problems during the pipeline, so checking error messages in the progress page's standard output section is important.","title":"Usage"},{"location":"usage/#usage","text":"Usage Adding a job Parameters File path input Input directory file structure Parameter Descriptions Progress Page What happens if an error occurs during the run? What happens when a job is completed? Output Page Files Produced Data Visualisation Variants Found Plots Produced From Pipeline","title":"Usage"},{"location":"usage/#adding-a-job","text":"To begin the process of adding a job, click the 'Add Job' button located underneath the Jobs Queue on the home page.","title":"Adding a job"},{"location":"usage/#parameters","text":"Input the necessary parameters (see Parameter Descriptions below). Parameters required for any type of job run are denoted with an asterix (*). For the file path inputs, please enter absolute paths. See below for help.","title":"Parameters"},{"location":"usage/#file-path-input","text":"Folders and files should be inputted by their file paths. File paths can be retrieved by running 'pwd' in the appropriate folder on any terminal. Folders may also be referred to as directories. File paths should start with \u201c/\u201d (Mac or Linux) or \u201cC:\\\u201d (Windows). If you have not worked with navigating folders and files in the terminal before, take a look at this resource: https://www.earthdatascience.org/courses/intro-to-earth-data-science/python-code-fundamentals/work-with-files-directories-paths-in-python/. For example: $ pwd # get file path of current directory /Users/YOURNAME $ ls # list contents of current directory folder1 folder2 file1 documents $ cd documents # change current directory to documents $ pwd /Users/YOURNAME/documents $ cd inputFolder # change current directory to your input folder $ ls # check contents of folder are correct fast5_fail fast5_pass fastq_fail fastq_pass sample-barcode.csv sequencing_summary.txt $ pwd # obtain file path you will input into interARTIC /Users/YOURNAME/documents/inputFolder Note: your input folder may not be located in documents folder. Simply navigate, using these commands, to inside your input folder and obtain the file path.","title":"File path input"},{"location":"usage/#input-directory-file-structure","text":"You must rename each folder/ file in the input folder to: fast5_pass, fast5_fail, fastq_pass, fastq_fail, sequencing_summary.txt, sample-barcode.csv If a single sample is being run through the pipeline: If available, the file path for a read file should be inputted. If unavailable, the artic gather/demultiplex command will generate one. If multiple samples are being run through the pipeline: A CSV file containing sample names and barcodes should be placed in the input folder and named \u2018sample-barcode.csv\u2019. A sample CSV file is below: A sample file structure is as below: input_directory/ fast5_pass/ A_10.fast5 A_11.fast5 fast5_fail/ A_0.fast5 A_1.fast5 fastq_pass/ B_0.fastq B_1.fastq fastq_fail/ B_10.fastq B_11.fastq sample-barcode.csv sequencing_summary.txt readfile.fastq #optional file primer-schemes/ nCoV-2019/ V1/ nCoV-2019.log nCoV-2019.pdf nCoV-2019.pickle nCoV-2019.reference.fasta nCoV-2019.reference.fasta.fai nCoV-2019.scheme.bed nCoV-2019.svg nCoV-2019.tsv","title":"Input directory file structure"},{"location":"usage/#parameter-descriptions","text":"You can customize the parameters by typing into the respective text box. Job name: A unique name for your job, so you may identify your output files with it. Pipeline: Select the pipeline within ARTIC that you wish to run your data files through. Input folder: Enter the file path to your main data folder. This folder contains folders such as fast5_pass, fastq_pass, etc. Read file: If you are inputting data files for a single sample run where you have a file ending in \u201c.fastq\u201d already made, input the file path for this read file. You may get this error in your output, but this can be ignored: Include screenshot of harmless error that might occur Output folder: If your chosen output folder is already created, enter the file path for this folder. Otherwise, you may enter a file path to a folder that doesn\u2019t exist yet. If you do this, ensure that the parent directory exists. For example, if you are inputting this file path as your output folder: path/to/file/hello/world, the folder \u201chello\u201d must already exist for the \u201cworld\u201d folder to be created. If this is not inputted, an output folder will be generated within the input folder. Primer scheme folder: Enter the file path to your primer schemes. This is the folder containing, for example, the folder nCoV-2019 which contains the V1, V2, etc folders. Primer scheme name: Enter the primer scheme name used for your nanopore sequencing run. Following a similar example to the previous parameter description, here you will enter a path such as nCoV-2019/V1. Primer/Barcode type: Enter the type of primer/barcode used. Either select from the options available or enter the name of the primer/barcode in the text box. This is only used for folder-naming purposes. Minimum/Maximum length: If you selected from the available options in the primer/barcode type section, you may find the minimum and maximum length already filled out. If not, set this to the minimum/maximum length of your primers. Thread usage/Normalise: Change the prefilled values if you wish. Please note that changing the threads/normalise values, they are changed globally for all commands. Single or Multiple samples: Select the appropriate option. When you are confident that your parameter selections are correct, click on the \u201cSubmit Job(s)\u201d button. You will be redirected to the progress page after clicking this button.","title":"Parameter Descriptions"},{"location":"usage/#progress-page","text":"The progress page displays the stream of standard output being produced by your job run. Here you can see which commands are currently running and any errors that occur. Each job run has its own progress page which can be accessed via the home page or parameters page by clicking on the job name in the jobs queue. For each job, the progress page will display: The job name The job's place in queue An 'Abort Job' button The overall progress of the job in the form of a progress bar and the number of steps remaining in the pipeline A 'View Job Parameters here' button The current standard output obtained from the job The 'View Job Parameters here' button, when clicked, will display the job's parameters that have been entered by the user. The 'Abort Job' button can be used to terminate the job. A confirmation window will appear when you click on the abort button. If you continue, you will then be asked to confirm whether you wish to delete the files created by the job. After this, you will then be directed back to the home page.","title":"Progress Page"},{"location":"usage/#what-happens-if-an-error-occurs-during-the-run","text":"If an error occurs during a run, a red notification will appear. You can either let the job continue to run, or click the \u2018Re-run\u2019 button. Harmless errors sometimes occur in the ARTIC pipeline, so it may be worth waiting for the run to finish and then assessing your output. Clicking the \u2018Re-run\u2019 button will allow you to abort the currently running job and re-run the job with editted parameters. A confirmation window will appear when you click on the \u2018Re-run\u2019 button asking you to confirm that you wish to abort the current job and whether to delete the files created by the job. You will then be redirected to the parameters page where the information from the job in question will be automatically filled in. You can make any changes necessary, and the new job will be added to the end of the queue following submission.","title":"What happens if an error occurs during the run?"},{"location":"usage/#what-happens-when-a-job-is-completed","text":"When a job is completed, a \u2018Go to Output\u2019 button will appear at the top of the page. Click the button to be redirected to the output page. The job will also be moved to the 'Completed Jobs' list on the home page where you can click on the job name and be redirected to the output page for that job.","title":"What happens when a job is completed?"},{"location":"usage/#output-page","text":"The Output Page has two main sections, one for the files produced during the run, and the other for data visualisation to enable a fast quality check of the sample. At the bottom of the page, there is a 'Go to Progress' button which will redirect you to the progress page of the job if you click on it.","title":"Output Page"},{"location":"usage/#files-produced","text":"This section contains a list of the files produced from the job, along with the location in which they were saved. This location should be the output folder that you inputted on the parameters page, or if you did not input one, it will be a folder named \u201coutput\u201d created inside your input folder.","title":"Files Produced"},{"location":"usage/#data-visualisation","text":"The \u201cData Visualisation\u201d section comprises two main parts. Variants found Plots produced from pipeline","title":"Data Visualisation"},{"location":"usage/#variants-found","text":"This section produces a simple graph/s from the data inside the <sample_name>.pass.vcf.gz file/s produced by the pipeline. If multiple samples are selected, multiple files will be produced (assuming all runs are successful). The 'Produce Graphs' button creates the graph/s. The vertical lines on the graph represent a variant, with the specific numerical position of the variant on the bottom axis. The mutation that occurred is labelled REF -> ALT, meaning that the nucleotide/s in the reference genome are on the left of the arrow, and the variant is on the right of the arrow. The height of the vertical line indicates the read depth of the specific variant. To download the graph, click on the 'Download' hyperlink in the lower-left corner of the graph of interest. These graphs, as mentioned, require interARTIC to read the <sample_name>.pass.vcf.gz file/s from the output folder. If there are no such files found, then no graphs will be produced. If you would not like the web app to access this, please click the \u201cDisable\u201d option under \u201cEnable VCF graphs to be generated?\u201d and click the 'Confirm' button. When you are no longer given the option to produce the graphs, and the status of the \u201cEnable VCF graphs to be generated?\u201d section is \u201cDisabled\u201d this is complete. If you would like to re-enable the graphs to be produced, just select \u201cEnable\u201d and once again confirm. The functionality should be restored. If no <sample_name>.pass.vcf.gz files are found in the output folder, the message \u201cVcf graph could not be made: No pass.vfc.gz file/s found in the output folder.\u201d will be displayed. As no files of the suitable format have been found, these graph/s cannot be produced. This may be due to errors or problems during the pipeline, so checking error messages in the progress page's standard output section is important.","title":"Variants Found"},{"location":"usage/#plots-produced-from-pipeline","text":"This section enables you to preview the mean amplicon read depth plots produced from the pipeline. The bar plot shows the mean amplicon read depth over the amplicons, while the box plot shows the mean amplicon read depth over the read groups. These plots are accessed from the output folder, and require a copy of them to be saved into the interARTIC web app\u2019s folder to enable previewing. If you would not like this to happen, please disable the feature by selecting the \u201cDisable\u201d option under the \u201cWould you like the plots to be previewed?\u201d section, and clicking the 'Confirm' button. The status should be updated to say \u201cDisabled\u201d and the plots will be automatically removed from the interARTIC web app's folder. To re-enable previewing, select \u201cEnable\u201d and press the 'Confirm' button again, and the option to preview graphs should be restored. If no barplot.png or boxplot.png files are found in the output folder, the message \u201cPlots cannot be previewed: No plots were found in the output folder.\u201d will be displayed. As no files in suitable format have been found, these graph/s cannot be previewed. This may be due to errors or problems during the pipeline, so checking error messages in the progress page's standard output section is important.","title":"Plots Produced From Pipeline"}]}