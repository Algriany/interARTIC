{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to InterARTIC InterARTIC is web application designed to automate the use of the Conda ARTIC pipeline. It is designed for use for the Nanopolish and Medaka pipelines. Overview","title":"Home"},{"location":"#welcome-to-interartic","text":"InterARTIC is web application designed to automate the use of the Conda ARTIC pipeline. It is designed for use for the Nanopolish and Medaka pipelines.","title":"Welcome to InterARTIC"},{"location":"#overview","text":"","title":"Overview"},{"location":"installation/","text":"Installation Dependencies Python 3.7 or above Celery v4.4.6 (cliffs) Redis Server v6.0.5 Miniconda Opening terminal Your operating systems command line will be used to install the dependencies. For Mac OS: Typing \u201cterminal\u201d into your spotlight search, then hit Return. For Windows: Typing \u201ccmd\u201d into your search bar in the Start menu, then hit Enter. For Linux: Entering Ctrl+Alt+T Installing Python and pip In order to use InterARTIC, you\u2019ll need Python and its package manager pip installed on your system. Check if they're already installed by entering the following into your command prompt: python --version pip --version If Python is not installed, go to: https://www.python.org/downloads/ and follow the prompts there. If you have just installed Python, it will likely have also installed pip. Check that it is installed, and upgrade if necessary. pip --version pip install --upgrade pip If pip is not installed, go to: https://pip.pypa.io/en/stable/installing/ and follow the prompts there. Installing miniconda The miniconda installation guide can be found here: https://conda.io/projects/conda/en/latest/user-guide/install/index.htm We suggest you undergo the 'Regular Installation' process. Installing the ARTIC pipeline Enter the following into your command prompt: git clone https://github.com/artic-network/artic-ncov2019.git cd artic-ncov2019 conda env remove -n artic-ncov2019 conda env create -f environment.yml Installing the Redis Server and Celery To install the Redis Server https://redis.io/topics/quickstart , enter the following into your command prompt: bash run-redis.sh Installing Python packages for Redis, Celery and Flask To install the Python packages for Redis, Celery and Flask, enter the following into your command prompt: pip install celery==4.4.6 redis==3.5.3 flask Installing InterARTIC Clone the repository from github by entering the following commands into your terminal. git clone https://github.com/tthnguyen11/SARS-CoV-2-NanoporeAnalysisWebApp.git Setting Up InterARTIC Job Concurrency By default, job concurrency is turned off and the automatic and manual setups will allow one job to be run at a time. If you wish to turn concurrency on and run multiple jobs at a time, the please run the Concurrency Manual setup , which will allow the all the CPUs available on your machine to be used to run jobs. Note that running jobs concurrently will likely slow down the speed of your machine. Automatic setup To start InterARTIC, navigate to the directory where the repository was cloned and enter the following command into your command prompt: cd SARS-CoV-2-NanoporeAnalysisWebApp bash run.sh <terminal type> # Terminal types: macos, xterm, konsole Manual setup If your terminal is not listed, enter the following commands into your command prompt: cd SARS-CoV-2-NanoporeAnalysisWebApp bash run-redis.sh & conda activate artic-ncov2019; celery worker -A main.celery --concurrency=1 --loglevel=info & python3 main.py Concurrency Manual setup If you wish to turn job concurrency on, enter the following commands into your command prompt: cd SARS-CoV-2-NanoporeAnalysisWebApp bash run-redis.sh & conda activate artic-ncov2019; celery worker -A main.celery --loglevel=info & python3 main.py Running InterARTIC Navigate to your browser and go to http://127.0.0.1:5000 to access InterARTIC.","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#dependencies","text":"Python 3.7 or above Celery v4.4.6 (cliffs) Redis Server v6.0.5 Miniconda","title":"Dependencies"},{"location":"installation/#opening-terminal","text":"Your operating systems command line will be used to install the dependencies. For Mac OS: Typing \u201cterminal\u201d into your spotlight search, then hit Return. For Windows: Typing \u201ccmd\u201d into your search bar in the Start menu, then hit Enter. For Linux: Entering Ctrl+Alt+T","title":"Opening terminal"},{"location":"installation/#installing-python-and-pip","text":"In order to use InterARTIC, you\u2019ll need Python and its package manager pip installed on your system. Check if they're already installed by entering the following into your command prompt: python --version pip --version If Python is not installed, go to: https://www.python.org/downloads/ and follow the prompts there. If you have just installed Python, it will likely have also installed pip. Check that it is installed, and upgrade if necessary. pip --version pip install --upgrade pip If pip is not installed, go to: https://pip.pypa.io/en/stable/installing/ and follow the prompts there.","title":"Installing Python and pip"},{"location":"installation/#installing-miniconda","text":"The miniconda installation guide can be found here: https://conda.io/projects/conda/en/latest/user-guide/install/index.htm We suggest you undergo the 'Regular Installation' process.","title":"Installing miniconda"},{"location":"installation/#installing-the-artic-pipeline","text":"Enter the following into your command prompt: git clone https://github.com/artic-network/artic-ncov2019.git cd artic-ncov2019 conda env remove -n artic-ncov2019 conda env create -f environment.yml","title":"Installing the ARTIC pipeline"},{"location":"installation/#installing-the-redis-server-and-celery","text":"To install the Redis Server https://redis.io/topics/quickstart , enter the following into your command prompt: bash run-redis.sh","title":"Installing the Redis Server and Celery"},{"location":"installation/#installing-python-packages-for-redis-celery-and-flask","text":"To install the Python packages for Redis, Celery and Flask, enter the following into your command prompt: pip install celery==4.4.6 redis==3.5.3 flask","title":"Installing Python packages for Redis, Celery and Flask"},{"location":"installation/#installing-interartic","text":"Clone the repository from github by entering the following commands into your terminal. git clone https://github.com/tthnguyen11/SARS-CoV-2-NanoporeAnalysisWebApp.git","title":"Installing InterARTIC"},{"location":"installation/#setting-up-interartic","text":"","title":"Setting Up InterARTIC"},{"location":"installation/#job-concurrency","text":"By default, job concurrency is turned off and the automatic and manual setups will allow one job to be run at a time. If you wish to turn concurrency on and run multiple jobs at a time, the please run the Concurrency Manual setup , which will allow the all the CPUs available on your machine to be used to run jobs. Note that running jobs concurrently will likely slow down the speed of your machine.","title":"Job Concurrency"},{"location":"installation/#automatic-setup","text":"To start InterARTIC, navigate to the directory where the repository was cloned and enter the following command into your command prompt: cd SARS-CoV-2-NanoporeAnalysisWebApp bash run.sh <terminal type> # Terminal types: macos, xterm, konsole","title":"Automatic setup"},{"location":"installation/#manual-setup","text":"If your terminal is not listed, enter the following commands into your command prompt: cd SARS-CoV-2-NanoporeAnalysisWebApp bash run-redis.sh & conda activate artic-ncov2019; celery worker -A main.celery --concurrency=1 --loglevel=info & python3 main.py","title":"Manual setup"},{"location":"installation/#concurrency-manual-setup","text":"If you wish to turn job concurrency on, enter the following commands into your command prompt: cd SARS-CoV-2-NanoporeAnalysisWebApp bash run-redis.sh & conda activate artic-ncov2019; celery worker -A main.celery --loglevel=info & python3 main.py","title":"Concurrency Manual setup"},{"location":"installation/#running-interartic","text":"Navigate to your browser and go to http://127.0.0.1:5000 to access InterARTIC.","title":"Running InterARTIC"},{"location":"main/","text":"","title":"Main"},{"location":"usage/","text":"Usage Adding a job To begin to add a job, click the \u201cAdd Job\u201d button located underneath the Jobs Queue on the home page. Parameters Input the necessary parameters (see Parameter Descriptions below). Any parameters required for all runs are denoted with an asterix (*). For the file path inputs, please enter absolute paths. Input folders Folders should be inputted by their file paths. This can be retrieved by running pwd in the appropriate directory/ folder on any terminal. These should start with \u201c/\u201d or \u201cC:\\\u201d. If you have not worked with navigating folders and files in the terminal before, take a look at this resource: https://www.earthdatascience.org/courses/intro-to-earth-data-science/python-code-fundamentals/work-with-files-directories-paths-in-python/. For example: $ pwd /Users/YOURNAME $ cd documents # change directory to documents /Users/YOURNAME/documents $ cd inputFolder # change directory to input folder $ pwd /Users/YOURNAME/documents/inputFolder Input directory file structure You must rename each folder in the input folder to: fast5_pass, fast5_fail, fastq_pass, fastq_fail, sequencing_summary.txt If a single sample is being run through the pipeline: If available, the file path for a read file should be inputted. If unavailable, the artic gather/demultiplex command will generate one. If multiple samples are being run through the pipeline: A CSV file containing sample names and barcodes should be placed in the input folder and named \u2018sample-barcode.csv\u2019. ADD IN IMG A sample file structure is as below: input_directory/ fast5_pass/ A_30.fast5 A_31.fast5 fast5_fail/ A_0.fast5 A_1.fast5 fastq_pass/ B_0.fastq B_1.fastq fastq_fail/ B_10.fast5 B_11.fast5 primer-schemes/ IturiEBOV/ V1/ IturiEBOV.log IturiEBOV.pdf IturiEBOV.pickle IturiEBOV.reference.fasta IturiEBOV.reference.fasta.amb IturiEBOV.reference.fasta.ann IturiEBOV.reference.fasta.bwt IturiEBOV.reference.fasta.fai IturiEBOV.reference.fasta.pac IturiEBOV.reference.fasta.sa IturiEBOV.scheme.bed IturiEBOV.svg IturiEBOV.tsv sample-barcode.csv sequencing_summary.txt readfile Parameters You can customize the parameters by typing into the respective text box. Job name: A unique name for your job, so you may identify your output files with it. Pipeline: Select the pipeline within ARTIC that you wish to run your data files through. Input folder: Enter the file path to your main data folder. This folder contains folders such as fast5_pass, fastq_pass, etc. Read file: If you are inputting data files for a single sample run where you have a file ending in \u201c.fastq\u201d already made, input the file path for this read file. You may get this error in your output, but this can be ignored: Include screenshot of harmless error that might occur Output folder: If your chosen output folder is already created, enter the file path for this folder. Otherwise, you may enter a file path to a folder that doesn\u2019t exist yet. If you do this, ensure that the parent directory exists. For example, if you are inputting this file path as your output folder: path/to/file/hello/world, the folder \u201chello\u201d must already exist for the \u201cworld\u201d folder to be created. If this is not inputted, an output folder will be generated within the input folder. Primer scheme folder: Enter the file path to your primer schemes. This is the folder containing, for example, the folder nCoV-2019 which contains the V1, V2, etc folders. Primer scheme name: Enter the primer scheme name used for your nanopore sequencing run. Following a similar example to the previous parameter description, here you will enter a path such as nCoV-2019/V1. Primer/Barcode type: Enter the type of primer/barcode used. Either select from the options available or enter the name of the primer/barcode in the text box. This is only used for folder-naming purposes. Minimum/Maximum length: If you selected from the available options in the primer/barcode type section, you may find the minimum and maximum length already filled out. If not, set this to the minimum/maximum length of your primers. Thread usage/Normalise: Change the prefilled values if you wish. Please note that changing the threads/normalise values, they are changed globally for all commands. Single or Multiple samples: Select the appropriate option. When you are confident that your parameter selections are correct, click on the \u201cSubmit Job(s)\u201d button. You will be redirected to the progress page after clicking this button. Progress Page The progress displays the progress of the run in question. Each run will have its own progress page, which can be accessed either via the home page by clicking the job name in the queue. For each job, the progress page will display: The job name The place in queue The overall job progress in the form of a progress bar and the number of steps remaining in the pipeline The current output obtained from the job There is a View Parameters button that will display the parameters that have been entered for the job when clicked. There is an Abort Job button which can be used to terminate the job. A confirmation window will appear when you click on the abort button. If you continue, you will then be asked to confirm whether you wish to delete the files created by the job. After this, you will then be taken back to the home page where you can add a new job or view the currenty completed jobs. What happens if an error occurs during the run? If an error occurs during a run, a red notification will appear. You can either continue with the job as is, or click the \u2018Re-run\u2019 button. Clicking the \u2018Re-run\u2019 button will allow you to abort the currently running job. A confirmation window will appear when you click on the \u2018Re-run\u2019 button asking you to confirm that you wish to abort the current job and whether to delete the files created by the job. You will then be redirected to the parameters page where the information from the job in question will be automatically filled in. You can make any changes necessary, and the new job will be added to the end of the queue following submission. What happens when a job is completed? When a job is completed, a \u2018Go to Output\u2019 button will appear at the top of the page. Click the button to be redirected to the output page. Output Page","title":"Usage"},{"location":"usage/#usage","text":"","title":"Usage"},{"location":"usage/#adding-a-job","text":"To begin to add a job, click the \u201cAdd Job\u201d button located underneath the Jobs Queue on the home page.","title":"Adding a job"},{"location":"usage/#parameters","text":"Input the necessary parameters (see Parameter Descriptions below). Any parameters required for all runs are denoted with an asterix (*). For the file path inputs, please enter absolute paths.","title":"Parameters"},{"location":"usage/#input-folders","text":"Folders should be inputted by their file paths. This can be retrieved by running pwd in the appropriate directory/ folder on any terminal. These should start with \u201c/\u201d or \u201cC:\\\u201d. If you have not worked with navigating folders and files in the terminal before, take a look at this resource: https://www.earthdatascience.org/courses/intro-to-earth-data-science/python-code-fundamentals/work-with-files-directories-paths-in-python/. For example: $ pwd /Users/YOURNAME $ cd documents # change directory to documents /Users/YOURNAME/documents $ cd inputFolder # change directory to input folder $ pwd /Users/YOURNAME/documents/inputFolder","title":"Input folders"},{"location":"usage/#input-directory-file-structure","text":"You must rename each folder in the input folder to: fast5_pass, fast5_fail, fastq_pass, fastq_fail, sequencing_summary.txt If a single sample is being run through the pipeline: If available, the file path for a read file should be inputted. If unavailable, the artic gather/demultiplex command will generate one. If multiple samples are being run through the pipeline: A CSV file containing sample names and barcodes should be placed in the input folder and named \u2018sample-barcode.csv\u2019. ADD IN IMG A sample file structure is as below: input_directory/ fast5_pass/ A_30.fast5 A_31.fast5 fast5_fail/ A_0.fast5 A_1.fast5 fastq_pass/ B_0.fastq B_1.fastq fastq_fail/ B_10.fast5 B_11.fast5 primer-schemes/ IturiEBOV/ V1/ IturiEBOV.log IturiEBOV.pdf IturiEBOV.pickle IturiEBOV.reference.fasta IturiEBOV.reference.fasta.amb IturiEBOV.reference.fasta.ann IturiEBOV.reference.fasta.bwt IturiEBOV.reference.fasta.fai IturiEBOV.reference.fasta.pac IturiEBOV.reference.fasta.sa IturiEBOV.scheme.bed IturiEBOV.svg IturiEBOV.tsv sample-barcode.csv sequencing_summary.txt readfile","title":"Input directory file structure"},{"location":"usage/#parameters_1","text":"You can customize the parameters by typing into the respective text box. Job name: A unique name for your job, so you may identify your output files with it. Pipeline: Select the pipeline within ARTIC that you wish to run your data files through. Input folder: Enter the file path to your main data folder. This folder contains folders such as fast5_pass, fastq_pass, etc. Read file: If you are inputting data files for a single sample run where you have a file ending in \u201c.fastq\u201d already made, input the file path for this read file. You may get this error in your output, but this can be ignored: Include screenshot of harmless error that might occur Output folder: If your chosen output folder is already created, enter the file path for this folder. Otherwise, you may enter a file path to a folder that doesn\u2019t exist yet. If you do this, ensure that the parent directory exists. For example, if you are inputting this file path as your output folder: path/to/file/hello/world, the folder \u201chello\u201d must already exist for the \u201cworld\u201d folder to be created. If this is not inputted, an output folder will be generated within the input folder. Primer scheme folder: Enter the file path to your primer schemes. This is the folder containing, for example, the folder nCoV-2019 which contains the V1, V2, etc folders. Primer scheme name: Enter the primer scheme name used for your nanopore sequencing run. Following a similar example to the previous parameter description, here you will enter a path such as nCoV-2019/V1. Primer/Barcode type: Enter the type of primer/barcode used. Either select from the options available or enter the name of the primer/barcode in the text box. This is only used for folder-naming purposes. Minimum/Maximum length: If you selected from the available options in the primer/barcode type section, you may find the minimum and maximum length already filled out. If not, set this to the minimum/maximum length of your primers. Thread usage/Normalise: Change the prefilled values if you wish. Please note that changing the threads/normalise values, they are changed globally for all commands. Single or Multiple samples: Select the appropriate option. When you are confident that your parameter selections are correct, click on the \u201cSubmit Job(s)\u201d button. You will be redirected to the progress page after clicking this button.","title":"Parameters"},{"location":"usage/#progress-page","text":"The progress displays the progress of the run in question. Each run will have its own progress page, which can be accessed either via the home page by clicking the job name in the queue. For each job, the progress page will display: The job name The place in queue The overall job progress in the form of a progress bar and the number of steps remaining in the pipeline The current output obtained from the job There is a View Parameters button that will display the parameters that have been entered for the job when clicked. There is an Abort Job button which can be used to terminate the job. A confirmation window will appear when you click on the abort button. If you continue, you will then be asked to confirm whether you wish to delete the files created by the job. After this, you will then be taken back to the home page where you can add a new job or view the currenty completed jobs.","title":"Progress Page"},{"location":"usage/#what-happens-if-an-error-occurs-during-the-run","text":"If an error occurs during a run, a red notification will appear. You can either continue with the job as is, or click the \u2018Re-run\u2019 button. Clicking the \u2018Re-run\u2019 button will allow you to abort the currently running job. A confirmation window will appear when you click on the \u2018Re-run\u2019 button asking you to confirm that you wish to abort the current job and whether to delete the files created by the job. You will then be redirected to the parameters page where the information from the job in question will be automatically filled in. You can make any changes necessary, and the new job will be added to the end of the queue following submission.","title":"What happens if an error occurs during the run?"},{"location":"usage/#what-happens-when-a-job-is-completed","text":"When a job is completed, a \u2018Go to Output\u2019 button will appear at the top of the page. Click the button to be redirected to the output page.","title":"What happens when a job is completed?"},{"location":"usage/#output-page","text":"","title":"Output Page"}]}